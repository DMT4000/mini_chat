#!/usr/bin/env python3
"""
Direct test of FAISS index and document retrieval.
This script tests the core functionality without complex imports.
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

def test_faiss_index():
    """Test FAISS index directly."""
    print("ðŸ§ª Testing FAISS index directly...")
    
    try:
        # Check if FAISS index exists
        faiss_path = Path("faiss_index")
        if not faiss_path.exists():
            print("âŒ FAISS index not found")
            return False
        
        print(f"âœ… FAISS index found at: {faiss_path}")
        
        # List contents
        index_contents = list(faiss_path.iterdir())
        print(f"ðŸ“ Index contents: {[item.name for item in index_contents]}")
        
        # Check if required files exist
        required_files = ['index.faiss', 'index.pkl']
        for file in required_files:
            if (faiss_path / file).exists():
                print(f"âœ… {file} found")
            else:
                print(f"âŒ {file} missing")
                return False
        
        return True
        
    except Exception as e:
        print(f"âŒ Error checking FAISS index: {e}")
        return False

def test_openai_connection():
    """Test OpenAI connection."""
    print("\nðŸ§ª Testing OpenAI connection...")
    
    try:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("âŒ OPENAI_API_KEY not found in environment")
            return False
        
        if api_key == "your_openai_api_key_here":
            print("âŒ OPENAI_API_KEY not properly set")
            return False
        
        print(f"âœ… OPENAI_API_KEY found (length: {len(api_key)})")
        return True
        
    except Exception as e:
        print(f"âŒ Error checking OpenAI connection: {e}")
        return False

def test_langchain_imports():
    """Test LangChain imports."""
    print("\nðŸ§ª Testing LangChain imports...")
    
    try:
        from langchain_community.vectorstores import FAISS
        print("âœ… FAISS import successful")
        
        from langchain_openai import OpenAIEmbeddings
        print("âœ… OpenAIEmbeddings import successful")
        
        return True
        
    except ImportError as e:
        print(f"âŒ Import error: {e}")
        return False
    except Exception as e:
        print(f"âŒ Unexpected error: {e}")
        return False

def test_faiss_loading():
    """Test loading FAISS index."""
    print("\nðŸ§ª Testing FAISS index loading...")
    
    try:
        from langchain_community.vectorstores import FAISS
        from langchain_openai import OpenAIEmbeddings
        
        # Initialize embeddings
        embeddings = OpenAIEmbeddings()
        print("âœ… OpenAI embeddings initialized")
        
        # Load FAISS index
        retriever = FAISS.load_local(
            "faiss_index", 
            embeddings, 
            allow_dangerous_deserialization=True
        ).as_retriever()
        print("âœ… FAISS index loaded successfully")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error loading FAISS index: {e}")
        return False

def test_document_search():
    """Test document search functionality."""
    print("\nðŸ§ª Testing document search...")
    
    try:
        from langchain_community.vectorstores import FAISS
        from langchain_openai import OpenAIEmbeddings
        
        # Load index
        embeddings = OpenAIEmbeddings()
        retriever = FAISS.load_local(
            "faiss_index", 
            embeddings, 
            allow_dangerous_deserialization=True
        ).as_retriever()
        
        # Test specific queries
        test_queries = [
            "workstream 5",
            "ws5", 
            "wellness app development",
            "workstreams objectives",
            "Fuxion products",
            "cronograma timeline"
        ]
        
        print("Testing document queries:")
        for query in test_queries:
            print(f"\nðŸ” Query: {query}")
            try:
                docs = retriever.get_relevant_documents(query)
                print(f"  âœ… Found {len(docs)} documents")
                
                if docs:
                    # Show top result details
                    top_doc = docs[0]
                    source = getattr(top_doc, 'metadata', {}).get('source', 'Unknown')
                    content_length = len(getattr(top_doc, 'page_content', ''))
                    print(f"  ðŸ“„ Top result source: {source}")
                    print(f"  ðŸ“ Content length: {content_length} characters")
                    
                    # Show content preview
                    content_preview = getattr(top_doc, 'page_content', '')[:200] + "..."
                    print(f"  ðŸ“– Content preview: {content_preview}")
                else:
                    print("  âŒ No documents found")
                    
            except Exception as e:
                print(f"  âŒ Error searching for '{query}': {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error in document search: {e}")
        return False

def test_workstream_specific():
    """Test specific workstream queries that were failing in the chat."""
    print("\nðŸ§ª Testing workstream-specific queries...")
    
    try:
        from langchain_community.vectorstores import FAISS
        from langchain_openai import OpenAIEmbeddings
        
        # Load index
        embeddings = OpenAIEmbeddings()
        retriever = FAISS.load_local(
            "faiss_index", 
            embeddings, 
            allow_dangerous_deserialization=True
        ).as_retriever()
        
        # Test the exact query from the chat
        chat_query = "si al workstream 5?"
        print(f"ðŸ” Testing chat query: {chat_query}")
        
        docs = retriever.get_relevant_documents(chat_query)
        print(f"âœ… Found {len(docs)} documents for chat query")
        
        if docs:
            print("\nðŸ“‹ Document details:")
            for i, doc in enumerate(docs[:3]):
                source = getattr(doc, 'metadata', {}).get('source', 'Unknown')
                content = getattr(doc, 'page_content', '')
                
                print(f"\n  Document {i+1}:")
                print(f"    Source: {source}")
                print(f"    Content length: {len(content)} characters")
                
                # Look for workstream information
                if 'workstream' in content.lower() or 'ws' in content.lower():
                    print(f"    âœ… Contains workstream information")
                    
                    # Extract workstream numbers
                    import re
                    ws_matches = re.findall(r'ws\s*(\d+)', content.lower())
                    if ws_matches:
                        print(f"    ðŸ”¢ Workstreams found: {ws_matches}")
                    
                    # Show relevant content
                    relevant_lines = [line for line in content.split('\n') 
                                    if 'workstream' in line.lower() or 'ws' in line.lower()]
                    if relevant_lines:
                        print(f"    ðŸ“ Relevant lines:")
                        for line in relevant_lines[:3]:
                            print(f"      {line.strip()}")
                else:
                    print(f"    âŒ No workstream information found")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error in workstream-specific testing: {e}")
        return False

def main():
    """Main test function."""
    print("ðŸš€ Starting Direct FAISS Test")
    print("=" * 80)
    
    tests = [
        ("FAISS Index Check", test_faiss_index),
        ("OpenAI Connection", test_openai_connection),
        ("LangChain Imports", test_langchain_imports),
        ("FAISS Loading", test_faiss_loading),
        ("Document Search", test_document_search),
        ("Workstream Specific", test_workstream_specific)
    ]
    
    results = []
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            success = test_func()
            results.append((test_name, success))
        except Exception as e:
            print(f"âŒ Test failed with exception: {e}")
            results.append((test_name, False))
    
    # Summary
    print(f"\n{'='*80}")
    print("ðŸ“Š TEST RESULTS SUMMARY")
    print("=" * 80)
    
    passed = 0
    total = len(results)
    
    for test_name, success in results:
        status = "âœ… PASSED" if success else "âŒ FAILED"
        print(f"{status}: {test_name}")
        if success:
            passed += 1
    
    print(f"\nOverall: {passed}/{total} tests passed")
    
    if passed == total:
        print("ðŸŽ‰ All tests passed! Document retrieval is working correctly.")
    else:
        print("âš ï¸ Some tests failed. Check the output above for details.")
        
        if passed >= 3:  # If most tests passed
            print("\nðŸ”§ The core functionality seems to be working.")
            print("The issue might be in the agent workflow integration.")
        else:
            print("\nðŸ”§ Core issues detected:")
            print("1. Check FAISS index integrity")
            print("2. Verify OpenAI API key and connection")
            print("3. Check LangChain dependencies")

if __name__ == "__main__":
    main()
